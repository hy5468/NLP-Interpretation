# NLP-Interpretation
A paper list for interpreting neural networks in natural language processing.

## Review

- **[arXiv-16]**: The mythos of model interpretability. [[paper]](https://arxiv.org/pdf/1606.03490.pdf) [[book]](https://dl.acm.org/doi/pdf/10.1145/3236386.3241340)

- **[arXiv-17]**: Towards A Rigorous Science of Interpretable Machine Learning. [[paper]](https://arxiv.org/pdf/1702.08608.pdf)

- **[blog]**: The Problem of Faithfulness in (Neural Network) NLP Interpretations. [[blog]](https://medium.com/@alonjacovi/the-problem-of-faithfulness-in-neural-network-nlp-interpretations-ee98d7027cbd)

- **[ZJU]**: 模型可解释性关键技术、应用及其安全性研究综述. [[paper]](https://nesa.zju.edu.cn/download/%E6%A8%A1%E5%9E%8B%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E3%80%81%E5%BA%94%E7%94%A8%E5%8F%8A%E5%85%B6%E5%AE%89%E5%85%A8%E6%80%A7%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0.pdf)

## Workshop

- **[EMNLP-18]**: BlackboxNLP 2018: Analyzing and interpreting neural networks for NLP. [[workshop]](https://www.aclweb.org/anthology/events/emnlp-2018/#w18-54) 

- **[ACL-19]**: BlackboxNLP 2019: Analyzing and interpreting neural networks for NLP. [[workshop]](https://www.aclweb.org/anthology/events/acl-2019/#w19-48)

## Paper

### Probing Task Method

- **[ACL-16]**: Does String-Based Neural MT Learn Source Syntax? [[paper]](https://www.aclweb.org/anthology/D16-1159.pdf)

- **[EMNLP-16]**: Analyzing linguistic knowledge in sequential model of sentence. [[paper]](https://www.aclweb.org/anthology/D16-1079.pdf)

- **[ICLR-17]**: Fine-grained analysis of sentence embeddings using auxiliary prediction tasks. [[paper]](https://openreview.net/pdf?id=BJh6Ztuxl)

- **[ACL-17]**: What do Neural Machine Translation Models Learn about Morphology? [[paper]](https://www.aclweb.org/anthology/P17-1080.pdf)

- **[EACL-17]**: How Grammatical is Character-level Neural Machine Translation? Assessing MT Quality with Contrastive Translation Pairs. [[paper]](https://www.aclweb.org/anthology/E17-2060.pdf)

- **[IJCNLP-17]**: Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks. [[paper]](https://www.aclweb.org/anthology/I17-1001.pdf)

- **[Machine Translation-17]**: The representational geometry of word meanings acquired by neural machine translation models. [[paper]](https://link.springer.com/article/10.1007%2Fs10590-017-9194-2)

- **[ACL-18]**: What you can cram into a single vector: Probing sentence embeddings for linguistic properties. [[paper]](https://www.aclweb.org/anthology/P18-1198.pdf)

- **[ACL-18]**: LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better. [[paper]](https://www.aclweb.org/anthology/P18-1132.pdf)

- **[EMNLP-18]**: Dissecting Contextual Word Embeddings: Architecture and Representation. [[paper]](https://arxiv.org/pdf/1808.08949.pdf)

- **[EMNLP-18-workshop]**: An Analysis of Encoder Representations in Transformer-Based Machine Translation. [[paper]](https://www.aclweb.org/anthology/W18-5431.pdf)

- **[ICLR-19]**: What do you learn from context? Probing for sentence structure in contextualized word representations. [[paper]](https://arxiv.org/pdf/1905.06316.pdf)

- **[NAACL-19]**: A Structural Probe for Finding Syntax in Word Representations. [[paper]](https://www.aclweb.org/anthology/N19-1419.pdf)

- **[ACL-19]**: BERT Rediscovers the Classical NLP Pipeline. [[paper]](https://www.aclweb.org/anthology/P19-1452.pdf)

- **[ACL-19]**: What does BERT learn about the structure of language? [[paper]](https://www.aclweb.org/anthology/P19-1356.pdf)

- **[ACL-19]**: Assessing the Ability of Self-Attention Networks to Learn Word Order. [[paper]](https://www.aclweb.org/anthology/P19-1354.pdf)

- **[ACL-19-workshop]**: Open Sesame: Getting Inside BERT’s Linguistic Knowledge. [[paper]](https://www.aclweb.org/anthology/W19-4825.pdf)

- **[CIKM-19]**: How Does BERT Answer Questions? A Layer-Wise Analysis of Transformer Representations. [[paper]](https://arxiv.org/pdf/1909.04925.pdf)

- **[EMNLP-19]**: Designing and Interpreting Probes with Control Tasks. [[paper]](https://www.aclweb.org/anthology/D19-1275.pdf)

- **[EMNLP-19]**: Encoders Help You Disambiguate Word Sensesin Neural Machine Translation. [[paper]](https://www.aclweb.org/anthology/D19-1149.pdf)

### Visualization Method

- **[ICLR-16]**: Visualizing and Understanding Recurrent Networks. [[paper]](https://arxiv.org/pdf/1506.02078.pdf)

- **[NAACL-16]**: Visualizing and Understanding Neural Models in NLP. [[paper]](https://www.aclweb.org/anthology/N16-1082.pdf)

- **[arXiv-16]**: Representation of linguistic form and function in recurrent neural networks. [[paper]](https://arxiv.org/pdf/1602.08952.pdf)

- **[ACL-17]**: Visualizing and Understanding Neural Machine Translation. [[paper]](https://www.aclweb.org/anthology/P17-1106.pdf)

- **[IJCNLP-17]**: What does Attention in Neural Machine TranslationPay Attention to? [[paper]](https://www.aclweb.org/anthology/I17-1004.pdf)

- **[IJCAI-18]**: Visualisation and "Diagnostic Classi ers" Reveal how Recurrent and Recursive Neural Networks Process Hierarchical Structure. [[paper]](https://www.ijcai.org/Proceedings/2018/0796.pdf)

- **[IEEE-VIS-18]**: Seq2Seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models. [[paper]](https://arxiv.org/pdf/1804.09299.pdf)

- **[EMNLP-18-workshop]**: Extracting Syntactic Trees from Transformer Encoder Self-Attentions.[[paper]](https://www.aclweb.org/anthology/W18-5444.pdf)

- **[ACL-19-workshop]**: From Balustrades to Pierre Vinken:Looking for Syntax in Transformer Self-Attention. [[paper]](https://www.aclweb.org/anthology/W19-4827.pdf)

- **[ACL-19-workshop]**: What does BERT look at? An Analysis of BERT’s Attention. [[paper]](https://www.aclweb.org/anthology/W19-4828.pdf)

### 

###  Local Interpretable Model-agnostic Explanations (LIME)

- **[NAACL-16]**: "Why Should I Trust You?": Explaining the Predictions of Any Classifier. [[paper]](https://www.aclweb.org/anthology/N16-3020.pdf)

### Adversarial Attack Method

- **[arXiv-19]**: What does BERT Learn from Multiple-Choice Reading Comprehension Datasets? [[paper]](https://arxiv.org/pdf/1910.12391.pdf)

### Canonical Correlation Analysis

- **[NIPS-17]**: SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability. [[paper]](https://arxiv.org/pdf/1706.05806.pdf)

- **[ICML-19]**: Similarity of Neural Network Representations Revisited. [[paper]](https://arxiv.org/pdf/1905.00414.pdf)

- **[EMNLP-19]**: The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives. [[paper]](https://www.aclweb.org/anthology/D19-1448.pdf)

### Interpreting with Nearest Neighbors

- **[EMNLP-18-workshop]**: Interpreting Neural Networks with Nearest Neighbors. [[paper]](https://www.aclweb.org/anthology/W18-5416.pdf)

### Attention Interpretability

- **[NAACL-19]**: Attention is not Explanation. [[paper]](https://www.aclweb.org/anthology/N19-1357.pdf)

- **[arXiv-19]**: Attention Interpretability Across NLP Tasks. [[paper]](https://arxiv.org/pdf/1909.11218.pdf)

- **[ACL-19]**: Is Attention Interpretable? [[paper]](https://www.aclweb.org/anthology/P19-1282.pdf)

- **[EMNLP-19]**: Attention is not not Explanation. [[paper]](https://www.aclweb.org/anthology/D19-1002.pdf)

### Difficulty of Interpreting

- **[arXiv-17]**: The (Un)reliability of saliency methods. [[paper]](https://arxiv.org/pdf/1711.00867.pdf)

- **[ICML-18-workshop]**: On the Robustness of Interpretability Methods. [[paper]](https://arxiv.org/pdf/1806.08049.pdf)

- **[EMNLP-18]**: Pathologies of Neural Models Make Interpretations Difficult. [[paper]](https://arxiv.org/pdf/1804.07781.pdf)

- **[AAAI-19]**: Interpretation of Neural Networks is Fragile. [[paper]](https://arxiv.org/pdf/1710.10547.pdf)

### Explanation Evaluating

- **[NAACL-18]**: Comparing Automatic and Human Evaluation of Local Explanations for Text Classification. [[paper]](https://www.aclweb.org/anthology/N18-1097.pdf)

- **[ACL-19-workshop]**: Evaluating Recurrent Neural Network Explanations. [[paper]](https://www.aclweb.org/anthology/W19-4813.pdf)

### Other

- **[ACL-16-workshop]**: Explaining Predictions of Non-Linear Classifiers in NLP. [[paper]](https://arxiv.org/pdf/1606.07298)

- **[ICML-17]**: Understanding Black-box Predictions via Influence Functions. [[paper]](https://arxiv.org/pdf/1703.04730.pdf)

- **[WMT-18]**: An Analysis of Attention Mechanisms: The Case ofWord Sense Disambiguation in Neural Machine Translation. [[paper]](https://www.aclweb.org/anthology/W18-6304.pdf)

- **[EAMT-18]**: An analysis of source context dependency in neural machine translation. [[paper]](https://rua.ua.es/dspace/bitstream/10045/76048/1/EAMT2018-Proceedings_21.pdf)

- **[NIPS-18]**: Sanity Checks for Saliency Maps. [[paper]](https://arxiv.org/pdf/1810.03292.pdf)

- **[NIPS-18]**: Towards Robust Interpretability with Self-Explaining Neural Networks. [[paper]](https://arxiv.org/pdf/1806.07538)

- **[EMNLP-18]**: Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference. [[paper]](https://www.aclweb.org/anthology/D18-1537.pdf)

- **[AAAI-19]**: What Is One Grain of Sand in the Desert? Analyzing Individual Neurons in Deep NLP Models. [[paper]](https://arxiv.org/pdf/1812.09355.pdf)

- **[ICLR-19]**: Identifying and Controlling Important Neurons in Neural Machine Translation. [[paper]](https://arxiv.org/pdf/1811.01157.pdf)

- **[SIGIR-19-workshop]**: Do Transformer Attention Heads Provide Transparency in Abstractive Summarization? [[paper]](https://arxiv.org/pdf/1907.00570.pdf)

- **[ACL-19]**: Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned. [[paper]](https://arxiv.org/pdf/1905.09418.pdf)

- **[EMNLP-19]**: Towards Understanding Neural Machine Translation with Word Importance. [[paper]](https://arxiv.org/pdf/1909.00326.pdf)

